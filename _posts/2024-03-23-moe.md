---
layout: post
title: Mixture of expert explanation
date: 2024-03-23 7:00:00-0700
description: What's new behind Mixture of expert (MoE) technique?
tags: moe, models, explanation
giscus_comments: true
related_posts: false
featured: true
bibliography: 2024-03-23-moe.bib

toc:
    sidebar: left

---

***TLDR;*** In middle of the trend of scaling language model, Mixture of Expert (MoE) is a potential candidate to replace current scaling method and achieve comparable score to close-source LLMs like GPT-3.5, GPT-4, Claude, etc. Mixtral 8x7B is the latest open-source model from Mistral and took the spotlight on several open LLM benchmarks until this day (03/2024). In this blog, I will dig into the technique behind the success of MoE and try my best to give you all you need to know about MoE.

# What is Mixtral 8x7B
In the LLMs marathon, Mixtral 8x7B leads the open-source benchmark in various chatbot categories. It boasts triple the votes of the 2nd open-source LLM and trails behind GPT-4, OpenAI's strongest LLM (by ~ 45 points)<d-footnote>https://openlm.ai/chatbot-arena/</d-footnote>. Mixtral 8x7B is the next version of Mixtral 7B, developed by the Mistral AI<d-footnote>https://mistral.ai/</d-footnote> team. The difference between the Mixtral 8x7B and other transformer models is the high-quality of the sparse mixture of expert layers (MoE) attachment.

The table below presents the strength of Mixtral 8x7B when compared with LLaMA 2 (the latest open-source LLM from Meta) and GPT-3.5 (the most common LLM from OpenAI). 


{% include figure.liquid loading="eager" path="assets/img/posts/moe/mistral-report.png" class="img-fluid rounded z-depth-1" zoomable=true %}
Figure 1: Mixture of Experts report (source: [mixtral-of-experts](https://mistral.ai/news/mixtral-of-experts/))


Mixtral 8x7B supports upto 32k tokens in term of context length, pre-train natively with 5 natural languages (i.e English, French, Italian, German and Spanish), releases under Apache 2.0 and allows people to commercialize it.

Mixtral also proved to be good at coding, which achieved 40.2% on HumanEval. The development team also released an Instruct version of Mixtral, which has been optimized through fine-tuning and DPO to follow instructions given by the user.


