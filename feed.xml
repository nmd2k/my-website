<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://nmd2k.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nmd2k.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-21T09:12:02+00:00</updated><id>https://nmd2k.github.io/feed.xml</id><title type="html">blank</title><subtitle>:wave: Hi, my name is Dung (Dzung). I&apos;m a computer scientist working on AI for software systems. </subtitle><entry><title type="html">Mixture of expert explanation</title><link href="https://nmd2k.github.io/blog/2024/moe/" rel="alternate" type="text/html" title="Mixture of expert explanation"/><published>2024-03-23T00:00:00+00:00</published><updated>2024-03-23T00:00:00+00:00</updated><id>https://nmd2k.github.io/blog/2024/moe</id><content type="html" xml:base="https://nmd2k.github.io/blog/2024/moe/"><![CDATA[<p><strong><em>TLDR;</em></strong> In middle of the trend of scaling language model, Mixture of Expert (MoE) is a potential candidate to replace current scaling method and achieve comparable score to close-source LLMs like GPT-3.5, GPT-4, Claude, etc. Mixtral 8x7B is the latest open-source model from Mistral and took the spotlight on several open LLM benchmarks until this day (03/2024). In this blog, I will dig into the technique behind the success of MoE and try my best to give you all you need to know about MoE.</p> <h2 id="what-is-mixtral-8x7b">What is Mixtral 8x7B</h2> <p>In the LLMs marathon, Mixtral 8x7B leads the open-source benchmark in various chatbot categories. It boasts triple the votes of the 2nd open-source LLM and trails behind GPT-4, OpenAI’s strongest LLM, by ~ 45 points<d-footnote>https://openlm.ai/chatbot-arena/</d-footnote>. Mixtral 8x7B is the next version of Mixtral 7B, developed by the Mistral AI<d-footnote>https://mistral.ai/</d-footnote> team. The difference between the Mixtral 8x7B and other transformer models is the high-quality of the sparse mixture of expert layers (MoE) attachment.</p> <p>The table below presents the strength of Mixtral 8x7B when compared with LLaMA 2 (the latest open-source LLM from Meta) and GPT-3.5 (the most common LLM from OpenAI).</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/moe/mistral-report.png" sizes="95vw"/> <img src="/assets/img/posts/moe/mistral-report.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Mixture of Experts report (source: mixtral-of-experts<d-footnote>https://mistral.ai/news/mixtral-of-experts/</d-footnote>) </div> <p>Mixtral 8x7B supports upto 32k tokens in term of context length, pre-train natively with 5 natural languages (i.e English, French, Italian, German and Spanish), releases under Apache 2.0 and allows people to commercialize it.</p> <p>Mixtral also proved to be good at coding, which achieved 40.2% on HumanEval. The development team also released an Instruct version of Mixtral, which has been optimized through fine-tuning and DPO to follow instructions given by the user.</p> <h2 id="mixture-of-expert">Mixture of Expert</h2> <p>Behind the success of mixtral 8x7B is mixture of expert (MoE) architecture. MoE bring to current transformers model the ability to speed up twice inferencing and pre-training time <d-cite key="du2022glam"></d-cite> due to the unique token path in expert layer, which only require certain nodes to work at each time. There are some recent work that found out MoE with instruction tuning is promising <d-cite key="shen2023mixtureofexperts"></d-cite>, make MoE is highly potential architecture in the near future.</p> <h3 id="definition-of-sparse-model">Definition of Sparse model</h3> <p>The sparse model is a combination of 2 parts: (1) a gate network (router) and (2) expert layers.</p> <ul> <li><strong>Sparse of MoE layer:</strong> Instead of using a normal feed-forward neural layer (FFN) inside transformer encoder or decoder, they replace it by a Sparse of MoE layer which contain a define number of experts, which in practice is actually FFN, but they can be also replace with any complex layer.</li> </ul> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/moe/moe-layer.png" sizes="95vw"/> <img src="/assets/img/posts/moe/moe-layer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Illustration of an MoE layer. For each input x, the router will only select one expert to perform computations. The choice is based on the output of the gating network (dotted line). The expert layer returns the output of the selected expert (gray box) multiplied by the route gate value (softmax of the gating function output). (Source: <d-cite key="chen2022understanding"></d-cite>) </div> <ul> <li><strong>Gate network or Router:</strong> A router will be used to determine which token will be sent to which expert. We can also send tokens to more than 1 expert. By the route system of gate neural, after training, each expert can specialize in a specific portion of the data. <d-cite key="chen2022understanding"></d-cite></li> </ul> <p>We can denote the MoE layer as follow:</p> \[y = \sum^{n}_{i=1}G(x_i)E_i\] <p>where $G$ present Gate neural network and $E_i$ indicate expert $i^{th}$. With $M$ is total number of expert layers, if $n$ is equal to whole expert set ($n=M$), this is called soft routing ($T_i=[N]$, at the moment $i$, tokens is sent to $N$ expert). But soft routing is not efficiency compare with dense model, therefor “switch routing” are replaced to save computation time and make the gating network sparse.</p> <p>Inside MoE layer contain $M$ expert, switch routing model will pick 1 expert from $M$ at each time $T_i=[1]=argmax_m({h_m(x; \Theta)})$.</p> <p>In practice, if $G$ is 0, the correspond expert operation is saving from computing. In a traditional setup, a softmax function can be added as a gating function (Wg is a trainable weight matrix multiplied by the input x):</p> \[G_{\sigma}(x)=Softmax(x.W_g)\] <p>The Gate network is so good so far, however, you might encouter with the question <em>“what if the number of training token is spent to each expert not equal?”</em>. Yes, it does. To purchase load-balancing for all experts, in Shazzer’s work <d-cite key="shazeer2017outrageously"></d-cite>, they added an additional trainable noise, i.e., Gaussian noice $H(x)$. And a “keep” function to store only the top k values and set the rest to $-\infty$ cause the gate values to equal 0, denoted as $KeepTopK()$.</p> \[H(x)_i=(x.W_g)_i+StandardNormal().Softplus((x.W_{noise})_i)\] \[KeepTopK(v,k)_i = \begin{cases}v1 &amp;\text{if } v_i \text{ is in the top } k \text{ elements of } v \\-\infty &amp; \text{other}\end{cases}\] <p>Therefore: $G(x) = Softmax(KeepTopK(H(x), k))$</p> <blockquote> <p class="block-warning">Since we introduced discontinuities in the output of the Gating function, however, this is fine. Don’t take my words, Shazzer said, “they observed this to be no problem in practice at all, and they trained the gating network by simple back-propagation, along with the rest of the model.”</p> </blockquote> <h3 id="addressing-challenges-of-moe">Addressing Challenges of MoE</h3> <h4 id="discontinuities-in-routing">Discontinuities in Routing</h4> <p>While sparse routing model saves computation and greatly reduces the inference times, it also causes discontinuities in routing<d-cite key="shazeer2017outrageously"></d-cite>. From the beginning, they added independently Gaussian noise $H(x)_i = (x.W_g)_i + StandardNormal()$, but in practice, they showed that even a small perturbation of the gating network outputs may change the router behavior drastically.</p> <p>From there, we can think of a additional loss function to allow the expert to receive roughly equal numbers of training example. However, when the training example come with a discrete quantity, it can not be used in back propagation. The problem can be solved by adding a smooth transition (smooth estimator $Load(X)$) between different routing behaviors (to make the router more stable).</p> <h4 id="balancing-expert-utilization">Balancing Expert Utilization</h4> <p>From the beginning, all experts initialize the same weights and training algorithm is the same. It is hard for new gating network learn right feature since every router is zero. Therefore, a mistake in training at the beginning may cause the expert to amplify that mistake.</p> <p>In Zixiang’s work <d-cite key="chen2022understanding"></d-cite>, they investigate the effectiveness of the initialization into expert divergence, they analyze the MoE layer between a stage called exploration stage where start at $t=0$ and ends at $T_1 = [\eta^-1\sigma_0^0.5]$, at each time one expert from $M$ experts is picked and with the gating network remains nearly unchanged. Even under the same treatment condition in each expert node, result shows after the exploration stage the experts become specialized to some specific task only based on the initialization.</p>]]></content><author><name>Dung Nguyen Manh</name></author><category term="moe"/><category term="models"/><category term="explanation"/><summary type="html"><![CDATA[What's new behind Mixture of expert (MoE) mechanism? [Part 1]]]></summary></entry><entry><title type="html">Alternative dataset for Code Understanding and Generation</title><link href="https://nmd2k.github.io/blog/2023/the-vault/" rel="alternate" type="text/html" title="Alternative dataset for Code Understanding and Generation"/><published>2023-08-19T21:00:00+00:00</published><updated>2023-08-19T21:00:00+00:00</updated><id>https://nmd2k.github.io/blog/2023/the-vault</id><content type="html" xml:base="https://nmd2k.github.io/blog/2023/the-vault/"><![CDATA[<p><strong><em>TLDR;</em></strong> The Vault is a multilingual code-text dataset with over 40 million pairs covering 10 popular programming languages. It is the largest corpus containing parallel code-text data. By building upon <strong><a href="https://huggingface.co/datasets/bigcode/the-stack">The Stack</a></strong>, a massive raw code sample collection, The Vault offers a comprehensive and high-quality resource for advancing research in code understanding and generation. The dataset also comes with an accessible toolkit for supporting the community and encouraging customize and improvement.</p> <h1 id="demand-for-large-scale-resource-parallel">Demand for large-scale resource parallel</h1> <p>The urgency for a massive parallel dataset in the domain of AI for code (AI4Code) stems from the increasing need to improve the performance of large language models (LLMs) for code generation and understanding. Despite the proliferation of open-source code repositories, we found out that existing code-text pair datasets, like CONCODE, FunCom, and CodeSearchNet, are significantly smaller than raw code datasets (CodeParrot Github, The Stack, The Pile, etc).</p> <p>In our experiment, we believe this size disparity has become a bottleneck for training AI4Code LLMs that rely heavily on fine-tuning pre-trained models using parallel code-text datasets. While models using non-parallel data or raw source code files are growing rapidly, those using code-text pair data (or bimodal) and chunked unimodal data (which only contain a certain level of code snippet) are constrained by data size.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-vault/the-vault-1.png" sizes="95vw"/> <img src="/assets/img/posts/the-vault/the-vault-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 1: A comparison table between current available function parallel dataset </div> <p>Furthermore, existing datasets like CoDesc, PyMT5, etc are limited to specific programming languages, underscoring the need for more diverse and larger datasets. Additionally, data quality plays a significant role in enhancing AI model performance. Therefore, the introduction of a large, high-quality dataset that spans multiple programming languages could greatly encourage the advancement of AI4Code LLMs.</p> <h1 id="what-does-the-vault-provide">What does The Vault provide?</h1> <p>CoDesc highlights the challenges faced by many tasks in the domain AI4Code, primarily the difficulty due to the lack of a standard noise removal method besides the lack of large standard datasets suitable for training deep neural models. To address those obstacles, we present The Vault, an extensive parallel dataset of code and docstring/comment. Our aim is to contribute to the community by providing a vast resource for training and evaluating natural language processing models related to code.</p> <p>Additionally, we also openly share our cleaning techniques and pipeline to extract data from raw code files, which involve careful design and analysis to enhance the data quality and optimize the performance in downstream tasks. When we design our cleaning rule, besides the 13 heuristics rule to eliminate un-informative data, we also apply a deep learning approach is utilized to refine the matching score between the parallel elements of The Vault.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-vault/the-vault-2.png" sizes="95vw"/> <img src="/assets/img/posts/the-vault/the-vault-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Our dataset and code are available to the community, and can easily access via Github and HuggingFace. The toolkit can support the community in various aspects of code extraction, parsing, filtering, and cleaning:</p> <p><strong>Key features:</strong></p> <ul> <li>Code parser: the package using <a href="https://tree-sitter.github.io/">tree-sitter</a> to provide an easy tool to extract function, class, and comment from source code.</li> <li>Docstring parser: Parse the docstring to acquire information such as summary comment, parameters, return type, docstring styles, etc.</li> <li>Filtering module: the module that integrates 13 rule-based methods to clean the docstring.</li> </ul> <h1 id="what-tasks-can-be-constructed-using-the-vault">What Tasks can be constructed using The Vault?</h1> <h3 id="code-summarization">Code summarization</h3> <p>Code summarization aims to generate human-readable descriptions which are meaningful and accurately describe the purpose or function of given code snippets. The input is a piece of code, and the output is a concise summary explaining its functionality.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-vault/the-vault-3.png" sizes="95vw"/> <img src="/assets/img/posts/the-vault/the-vault-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Traditionally, models are trained on parallel datasets containing code and corresponding comments or descriptions. However, these datasets can be noisy and limited, with descriptions that can be ambiguous, incomplete, or even misleading. In The Vault, we carefully design a cleaning method to capture all the informative sections of the docstring/description in order to boost model performance in the summarization task.</p> <h3 id="code-search">Code search</h3> <p>Code search is the task of retrieving relevant code snippets given a natural language query. The input is a query in natural language, and the output is a set of code snippets that satisfy the query. Models for this task are typically trained or fine-tuned using code and associated natural language descriptions from parallel datasets where both will be embedded into a vector space and compare similarities with each other.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-vault/the-vault-4.png" sizes="95vw"/> <img src="/assets/img/posts/the-vault/the-vault-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>However, the effectiveness of a code search system heavily depends on the quality and diversity of its training data. Therefore, a well-curated, large, and diverse parallel dataset can significantly reduce this challenge and boost the model’s understanding of the relationship between code and natural language, resulting in better performance.</p> <h3 id="code-generation">Code generation</h3> <p>Code generation, or text-to-code, involves creating code based on a natural language description. The input is a description of a desired functionality, and the output is the code that performs that function. Models for this task are usually trained or fine-tuned on parallel datasets that contain pairs of natural language descriptions and their corresponding code. However, the existing datasets are often limited in their coverage of programming languages and can contain noisy or low-quality examples. A large, diverse, and high-quality parallel dataset can provide a broader range of examples and contexts, thereby improving the model’s ability to generate syntactically and semantically correct code across various languages and tasks.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-vault/the-vault-5.png" sizes="95vw"/> <img src="/assets/img/posts/the-vault/the-vault-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="more-advanced-problems">More advanced problems</h3> <p>When extracting the Vault, our focus extends beyond extracting parallel elements from the source code. We make an effort to extract as much metadata as possible, including function/class information. The metadata obtained from The Vault dataset, such as parameter descriptions and types found in docstrings, can be utilized to generate more detailed and informative code summaries. By leveraging this information, models can provide insights into the purpose, expected inputs, and outputs of the suggested code, enhancing the summarization process.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-vault/the-vault-6.png" sizes="95vw"/> <img src="/assets/img/posts/the-vault/the-vault-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>To extract docstring metadata, we have developed a docstring style parser capable of extracting information from each style. By leveraging the specific conventions and patterns used in various programming languages, models can gain a deeper understanding of these languages. This approach facilitates knowledge transfer and code comprehension across different programming paradigms, bridging the gap between them.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-vault/the-vault-7.png" sizes="95vw"/> <img src="/assets/img/posts/the-vault/the-vault-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 2: Example of 6 docstring styles </div> <h1 id="data-characteristics-inside-the-vault">Data characteristics inside The Vault</h1> <p>In this exciting new dataset, we have delved into the world of programming languages to uncover some fascinating insights.</p> <h2 id="comparison-with-existing-benchmarks">Comparison with Existing Benchmarks</h2> <p>The Vault stands out from existing datasets due to its extensive coverage of programming languages and its substantial size. The comparison is illustrated in the figure below.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-vault/the-vault-8.png" sizes="95vw"/> <img src="/assets/img/posts/the-vault/the-vault-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 3: Function set comparison properties with other available datasets </div> <p>After deduplication, we achieve over 34M pairs, which still significantly surpasses the scale of existing datasets. We split the data into 3 training sets (small, medium, and full), a validation set, and a test set.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-vault/the-vault-9.png" sizes="95vw"/> <img src="/assets/img/posts/the-vault/the-vault-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="data-characteristics-distributions">Data characteristics distributions</h2> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-vault/the-vault-10.png" sizes="95vw"/> <img src="/assets/img/posts/the-vault/the-vault-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The table above provides information about the dataset, highlighting the prevalence of Python and Java programming languages and the abundance of tokens within the dataset. Python and Java are the dominant languages in the dataset, indicating their popularity and widespread usage. The dataset encompasses a significant number of tokens, indicating a substantial amount of code and associated text.</p> <p>In analyzing each sample, the average token length for code hovers around 100 tokens, while docstrings exhibit remarkable brevity with an average of only 15 tokens.</p> <h2 id="docstring-styles">Docstring styles</h2> <p>Alongside typical docstrings that provide brief descriptions of the source code, many adhere to formatting and style conventions like Google, Jsdoc, and reST styles, among others.</p> <p>With our carefully designed toolkit, we have harnessed the power to parse these docstrings and extract valuable metadata, all while supporting a remarkable 11 prevalent docstring styles.</p> <p>Through extensive statistical analysis, we have gained insights into the prevalence of styled docstrings within the larger code-text dataset. It is fascinating to note that these styled docstrings constitute only a fraction of the entire corpus. However, this seemingly small fraction opens up a world of possibilities for advanced research. Imagine the potential of controlling docstring style during generation or crafting nuanced explanations for function parameters. This rich dataset provides a fertile ground for exploring and tackling these complex challenges.</p> <h1 id="unleash-your-curiosity-and-explore-further">Unleash Your Curiosity and Explore Further</h1> <p>We encourage you to explore the concepts and ideas presented in this blog post in more detail. You can find additional information through the resources provided below.</p> <p><strong><em>Paper</em>:</strong> More details about The Vault can be found in our <a href="https://arxiv.org/abs/2305.06156">research paper</a>.</p> <p><strong><em>GitHub</em>:</strong> Check out our source code and toolkit <a href="https://github.com/FSoft-AI4Code/TheVault/">HERE</a>.</p> <p><strong><em>Huggingface:</em></strong> Check out The Vault on <a href="https://huggingface.co/datasets/Fsoft-AIC/the-vault-function">HuggingFace hub</a>.</p> <p><strong><em>Feedback or Questions:</em></strong> Contact us at <a href="mailto:support.ailab@fpt.com">support.ailab@fpt.com</a>.</p> <h1 id="acknowledgments">Acknowledgments</h1> <p>We are grateful to have the support of the FPT Software AI Center in funding this project. For more details about our organization and its initiatives, please visit <strong><a href="https://www.fpt-aicenter.com/about/">https://www.fpt-aicenter.com/about/</a></strong>.</p>]]></content><author><name></name></author><category term="dataset,"/><category term="ai4code,"/><category term="multilingual"/><summary type="html"><![CDATA[A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation]]></summary></entry></feed>